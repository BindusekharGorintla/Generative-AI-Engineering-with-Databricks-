In-Context Learning with AI Playground
In this demo, we will show the importance of providing context when using generative AI models. First, we will prompt the
model to provide a response to a query that you know it has no knowledge of, e.g. proprietary information request, 
personal information request, obscure knowledge request. Then we will review the response for hallucinations, 
incorrect information, or lack of detail. In order to overcome this problem, we will provide the prompt a document or
reference that contains the correct information as supplemental information and ask the query again. Finally, we will review the new response for a change in detail given the new information as prompt context.

Learning Objectives:

By the end of this demo, you will be able to;

Identify scenarios where generative AI models may produce hallucinations in response to prompts.

Apply techniques to construct prompts that guide generative AI models to provide responses without hallucinations.

Augment prompts with additional context to improve the accuracy and relevance of responses generated by generative AI models.

Accessing the Playground
To use the AI Playground:

Select Playground from the left navigation pane under AI/ML.

Select the model to use.

Optional: Change the model parameters.

Optional: To compare the responses of multiple models, you can add endpoint and see the responses side-by-side.

playground

üö®Note: You have to clear the Playground history if you don‚Äôt want it in ‚Äúchat‚Äù (conversation) mode.

Simple Prompt Which Hallucinates
One situation in which LLM's will potentially hallucinate is when they're prompted for historical information for a person that doesn't actually exist, especially when the prompt implies other fictional facts to go along with the person. We can test this by providing a system prompt for an assistant we'd like to be specialized on biographical information and then provide a user prompt that requests information about such a person.

üìå IMPORTANT: The level of hallucination depends on the LLM used. For this demo, use Llama 4 Maverick to see the model's hallucination. If you use claude 3.7 Sonnet, you will see that the model hallucinates much less.

To get started, set the system prompt in the Playground to the following.

üí¨ System Prompt:

You are a helpful expert assistant specializing in providing product information for FakeTechCorp. You will be asked questions about specific products, and your job is to provide short, clear, and helpful responses. Be polite and informative at all times.

Now that we've provided a system prompt, let's see if we can generate a hallucination in a biography for a person we know doesn't exist, such as the following:

üí¨ Query:

Provide detailed specifications for FakeTechCorp's QuantumEdge Server 9000, including its processing capabilities, power consumption, and enterprise pricing structure.

Llama 4 Maverick Instruct in this example generates the following helpful biographical information in response to this request:

ü§ñ Response:

I'm happy to provide you with the specifications for the FakeTechCorp QuantumEdge Server 9000.
The QuantumEdge Server 9000 is a high-performance server designed for demanding enterprise applications. Here are its detailed specifications:
Processing Capabilities:
The QuantumEdge Server 9000 is powered by a dual-socket configuration, supporting up to 2x FakeTechCorp's latest generation Xeon-compatible QuantumCore processors.
Each processor features 32 cores and 64 threads, clocked at a base frequency of 3.2 GHz and a boost frequency of up to 4.0 GHz.
The server supports a maximum of 16 DIMM slots, accommodating up to 4 TB of DDR5 RAM with speeds of up to 6400 MHz.
Memory and Storage:
The server offers flexible storage options, including up to 8x 2.5-inch NVMe SSDs or 12x 3.5-inch SAS/SATA drives.
It supports FakeTechCorp's advanced storage technology, allowing for RAID configurations and high-speed data access.
Power Consumption:
The QuantumEdge Server 9000 is designed with efficiency in mind, featuring a redundant power supply configuration with 80 PLUS Titanium certified power supplies.
The server's typical power consumption is around 1200W, with a maximum power draw of 1600W under full load.
Enterprise Pricing Structure:
The QuantumEdge Server 9000 is available in several configurations, with pricing starting at $12,995 for a base model (2x 32-core processors, 256 GB DDR5 RAM, 2x 1 TB NVMe SSDs).
Mid-range configurations (2x 32-core processors, 512 GB DDR5 RAM, 4x 2 TB NVMe SSDs) are priced at $18,495.
High-end configurations (2x 32-core processors, 4 TB DDR5 RAM, 8x 4 TB NVMe SSDs) are available for $29,995.
FakeTechCorp offers discounts for bulk orders and enterprise customers, with customized pricing available for large deployments.
Additional Features:
The QuantumEdge Server 9000 includes advanced features such as integrated management tools, remote monitoring capabilities, and support for FakeTechCorp's AI-driven performance optimization software.
It is backed by FakeTechCorp's comprehensive warranty and support services, ensuring reliability and peace of mind for enterprise customers.```
As you can see, that's quite an impressive amount of product information that the LLM seems rather sure is correct about a product we just made up!

But suppose the QuantumEdge Server 9000 did actually exist, and we had access to real information about it. Can we provide some additional context‚Äîsuch as actual product specs retrieved from a vector database‚Äîto improve the response and ensure accuracy?

Let's find out!

Simple Prompt Which Does Not Hallucinate
One effective way to prevent LLMs from hallucinating is to set clear guidelines within the system prompt, explicitly instructing the model to refrain from generating information if it is not available in the provided context. This approach helps ensure the responses are accurate and grounded in provided data.

To illustrate this, we can configure a system prompt for a product information assistant and then pose a query about a fictional product to see how the model responds when it doesn't have relevant information.

üö® Note: It‚Äôs valuable to include a system prompt telling the model not to hallucinate or invent details.

üí¨ System Prompt:

You are a helpful expert assistant specializing in providing product information for FakeTechCorp. You will be asked questions about specific products. Your answers should only use the context that is provided. If you do not have information about a product, do not make up information; simply say that you do not know. Be polite and informative at all times.

üí¨ Query:

Provide detailed specifications for FakeTechCorp's QuantumEdge Server 9000, including its processing capabilities, power consumption, and enterprise pricing structure.

ü§ñ Response:

I'm sorry, but I do not have information about the specifications, power consumption, or pricing for the FakeTechCorp QuantumEdge Server 9000, as no context was provided. If you can share more details or official documentation, I'd be happy to assist further!

By clearly instructing the model not to fabricate details and only respond based on provided information, we significantly reduce the likelihood of hallucination. This approach ensures that the assistant provides reliable and trustworthy responses.

Augment Prompt with Additional Context
Let's augment the prompt with some additional context. In a typical RAG (Retrieval Augmented Generation) application, as discussed in the lecture, this context would be provided by looking up information in a database, typically using vector search based on an embedding model. After giving instructions to prevent generating hallucinated responses, the model can produce responses based on the context provided. Once this information is retrieved, it is then injected into the prompt as additional context. However, you can also experiment directly with this last step by simply providing additional context directly in the prompt in the Playground! This is still Retrieval Augmented Generation, but you are doing the Retrieval part manually. This is great for quickly testing new ideas and experimenting with various LLMs, prompts, and context formats.

To illustrate the impact of additional context, let's maintain the same system prompt but add more context in the user query.

üí¨ Query:

Provide detailed specifications for FakeTechCorp's QuantumEdge Server 9000, including its processing capabilities, power consumption, and enterprise pricing structure.

Context: "FakeTechCorp's enterprise server lineup currently includes the QuantumEdge Server 9000, PowerEdge 8500 series, and CloudScale rack solutions. The QuantumEdge Server 9000 features 64-core processors, up to 1TB RAM, with power consumption ranging from 650-850W depending on configuration. Base pricing starts at $45,000 with enterprise support packages available at $8,500/year."

This time around, the model gives us the following response:

ü§ñ Response:

I'm happy to provide you with the specifications for the FakeTechCorp QuantumEdge Server 9000.
The QuantumEdge Server 9000 is a high-performance server designed for demanding enterprise applications. Here are its detailed specifications:
Processing Capabilities:
- The QuantumEdge Server 9000 features 64-core processors, providing high processing power for enterprise workloads.
Memory:
- The server supports up to 1 TB of RAM, ensuring ample memory for large-scale applications.
Power Consumption:
- The power consumption of the QuantumEdge Server 9000 ranges from 650-850W, depending on the specific configuration.
Enterprise Pricing Structure:
- The base pricing for the QuantumEdge Server 9000 starts at $45,000.
- Enterprise support packages are available for $8,500 per year, providing comprehensive support and maintenance services for the server.
I don't have more detailed information about the QuantumEdge Server 9000's storage options, additional features, or other specifications beyond what's provided. If you need further details, I recommend contacting FakeTechCorp directly for more information.
As you can see, providing additional context significantly impacts the output, guiding the model to generate a response closely aligned with the given details. While some hallucination may still occur if the model tries to elaborate beyond the context, the incorporation of provided context ensures that the response remains relevant and grounded. With further prompt engineering, we could refine the responses to minimize hallucinations even more effectively.

Summary
In this demo, we used the Mosaic AI Playground to demonstrate a basic example of augmenting an LLM with additional context.

But how can we further generate improve our responses with better context, prompt engineering, or selecting alternative large language models? In the lab, you'll try out some examples of your own to see if you can both demonstrate and mitigate hallucinations using a combination of LLM selection, context injection, and prompt engineering, the basic tools of RAG application development.

¬© 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the Apache Software Foundation.

Privacy Policy | Terms of Use | Support

