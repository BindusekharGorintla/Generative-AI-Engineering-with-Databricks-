Preparing Data for RAG
In this demo, we will focus on ingesting PDF documents as a source for our retrieval process. First, we will ingest the PDF files and process them using self-managed vector search embeddings.

Learning Objectives:

By the end of this demo, you will be able to:

Split the data into chunks that are at least as small as the maximum context window of the LLM to be used later.

Describe how to appropriately choose an embedding model.

Compute embeddings for each of the chunks using a Databricks-managed embedding model.

Use the chunking strategy to divide up the context information to be provided to a model.

REQUIRED - SELECT CLASSIC COMPUTE
Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that Serverless is enabled by default.

Follow these steps to select the classic compute cluster:

Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use Serverless.

If your cluster is available, select it and continue to the next cell. If the cluster is not shown:

Click More in the drop-down.

In the Attach to an existing compute resource window, use the first drop-down to select your unique cluster.

NOTE: If your cluster has terminated, you might need to restart it in order to select it. To do this:

Right-click on Compute in the left navigation pane and select Open in new tab.

Find the triangle icon to the right of your compute cluster name and click it.

Wait a few minutes for the cluster to start.

Once the cluster is running, complete the steps above to select your cluster.

Requirements
Please review the following requirements before starting the lesson:

To run this notebook, you need to use one of the following Databricks runtime(s): 17.3.x-cpu-ml-scala2.13
Classroom Setup
Install required libraries.

Before starting the demo, run the provided classroom setup script. This script will define the configuration variables necessary for the demo. Execute the following cell:

Other Conventions:

Throughout this demo, we'll refer to the object DA. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:

Demo Overview
For this example, we will add research articles on GenerativeAI as PDFs from Arxiv resources page to our knowledge database.

genai-as-01-rag-pdf-self-managed-0

Here are all the detailed steps:

Use Spark to load the binary PDFs into our first table.
Use the unstructured library to parse the text content of the PDFs.
Use llama_index or langchain to split the texts into chunks.
Compute embeddings for the chunks.
Save our text chunks + embeddings in a Delta Lake table, ready for Vector Search indexing.
Databricks Lakehouse AI not only provides state of the art solutions to accelerate your AI and LLM projects but also to accelerate data ingestion and preparation at scale, including unstructured data like PDFs.

Extract PDF Content as Text Chunks
As the first step, we need to ingest PDF files and divide the content into chunks. PDF files are already downloaded during the course step and stored in datasets path.

Let's view the content of one of the articles.

This looks great. We'll now wrap it with a text_splitter to avoid having too big pages and create a Pandas UDF function to easily scale that across multiple nodes.

ðŸ“ŒNote: The pdf text isn't clean. To make it nicer, we could use a few extra LLM-based pre-processing steps, asking to remove irrelevant content like the list of chapters and to only keep the core text.


Enhance GenAI observability with MLflow Tracing: It is highly recommended that you enable MLflow Tracing when developing GenAI applications. Tracing provides a step-by-step analysis of your app's execution, helping you debug latency, cost, and quality issues. Traces can be used with Agent Evaluation to measure your app's quality, and the same Trace will be automatically captured once you deploy your application.
To enable tracing, run the following code:
%pip install -U mlflow
dbutils.library.restartPython()
Followed by this command in a separate notebook cell:
import mlflow
mlflow.llama_index.autolog()
Don't show me this again 
Try Serverless GPU Compute: Serverless GPU Compute provides fast, scalable access to GPUs without the need to configure or maintain a GPU cluster. In just a few clicks, users can seamlessly connect a GPU to a notebook and start training models directly within their Databricks lakehouse. Learn more
Don't show me this again 
 
ðŸ’¡ Chunking Overlap: Review the content chunks and pay attention to the sentence overlap between chunks. This was defined with SentenceSplitter above.

Embeddings with Foundation Model Endpoints
genai-as-01-rag-pdf-self-managed-4

Foundation Models are provided by Databricks, and can be used out-of-the-box.

Databricks supports several endpoint types to compute embeddings or evaluate a model:

A foundation model endpoint, provided by databricks (ex: llama3.3-70B, databricks-claude-3-7-sonnet...)
An external endpoint, acting as a gateway to an external model (ex: Azure OpenAI)
A custom, fine-tuned model hosted on Databricks model service
Open the Model Serving Endpoint page to explore and try the foundation models.

For this demo, we will use the foundation model GTE (embeddings) and llama3.3-70B (chat).


serving

How to Use Foundation Model API
Before computing the embeddings for the chunked text we created before, let's quickly go over how to use Foundation Model API.

ðŸš¨ Important: You will need Foundation Model API access for this section and the rest of the demo.

Compute Chunking Embeddings
The last step is to now compute an embedding for all our documentation chunks. Let's create an udf to compute the embeddings using the foundation model endpoint.

ðŸ“Œ Note: This part would typically be setup as a production-grade job, running as soon as a new documentation page is updated. This could be setup as a Delta Live Table pipeline to incrementally consume updates.

Save Embeddings to a Delta Table
Now that the embeddings are ready, let's create a Delta table and store the embeddings in this table.

Clean up Classroom
ðŸš¨ Warning: Please refrain from deleting the tables created in this demo, as they are required for upcoming demos.

Conclusion
In this demo, we demonstrated how to ingest and process documents for a RAG application. The first step was to extract text chunks from PDF documents. Then, we created embeddings using a foundation model. This process includes setting up an endpoint and computing embeddings for the chunks. In the final step, we stored computed embeddings in a Delta table.

Â© 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the Apache Software Foundation.
